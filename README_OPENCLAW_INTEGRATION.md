# üéôÔ∏è REM + OpenClaw: Complete AI Assistant System

**Status:** ‚úÖ Production Ready | **Date:** Feb 21, 2026 | **Version:** 1.0.0

---

## üìå What is This?

**REM (Remote Engagement Manager)** is a local AI assistant that runs entirely on your machine with no cloud dependencies. It now integrates seamlessly with **OpenClaw**, a multi-channel AI gateway.

### REM Provides:
- üß† **LLM** ‚Äî Text generation using local Ollama (Llama 3.2)
- üîä **TTS** ‚Äî Text-to-speech via GPT-SoVITS (optional)
- üé§ **ASR** ‚Äî Speech-to-text using Whisper
- üåê **Web Control** ‚Äî Open YouTube/Google from voice commands
- üí¨ **Chat** ‚Äî Conversation with memory/history
- üéôÔ∏è **Voice Modes** ‚Äî Always-listening assistant

### OpenClaw Integration:
- üåâ **HTTP API Bridge** ‚Äî REM services available via REST
- ü§ñ **Agent Support** ‚Äî Full integration with OpenClaw's AI routing
- üîå **Skill Registration** ‚Äî REM appears as OpenClaw "rem-local" skill
- üîÑ **Bidirectional** ‚Äî OpenClaw ‚Üî REM communication

---

## üöÄ Quick Start (5 Minutes)

### Prerequisites
```bash
# Ensure you have:
# - Python 3.8+ installed
# - Ollama running: ollama serve (in another terminal)
# - Virtual environment activated: .\.venv\Scripts\Activate.ps1
```

### Run These Commands
```bash
# Navigate to project
cd "C:\Users\hardi\Chat bot\Rem_project"

# Verify services
rem health

# Test LLM
rem llm "What is Python?"

# Test web control
rem web "search youtube for python tutorials"

# Test speech (optional)
rem tts "Hello world"

# Start interactive voice mode
rem voice listen
```

**All of the above should work immediately!**

---

## üìñ Complete Usage Guide

### 1Ô∏è‚É£ Direct REM Commands (Simplest)

No setup required, just use the `rem` command:

```bash
# Text generation
rem llm "Explain quantum computing in simple terms"

# Chat with conversation memory
rem llm-chat "Hello, how are you?"
rem llm-chat "What am I doing?"  # Remembers previous context

# Text to speech (if SoVITS available)
rem tts "The quick brown fox"

# Speech recognition
rem asr "audio/my_voice.wav"

# Web searches
rem web "search youtube for machine learning"
rem web "google what is AI"

# Interactive voice mode
rem voice listen           # Single voice input & response
rem voice wake_word        # Waits for "Hey REM" trigger
rem voice always_on        # Always listening (experimental)

# Service health
rem health
```

### 2Ô∏è‚É£ HTTP Bridge API (for OpenClaw)

Start the HTTP RPC server:

```bash
# Start bridge (default: http://127.0.0.1:8765)
rem bridge

# Or with custom settings
rem bridge --host 0.0.0.0 --port 9000
```

Then call it from anywhere:

```bash
# Health check
curl http://127.0.0.1:8765/health

# Get service info
curl http://127.0.0.1:8765/info

# LLM generation
curl -X POST http://127.0.0.1:8765/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is AI?"}'

# Chat
curl -X POST http://127.0.0.1:8765/llm/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hi, how are you?"}'

# Web control
curl -X POST http://127.0.0.1:8765/web/control \
  -H "Content-Type: application/json" \
  -d '{"text": "search youtube for python"}'

# Generic RPC call
curl -X POST http://127.0.0.1:8765/call \
  -H "Content-Type: application/json" \
  -d '{"service": "llm_generate", "params": {"prompt": "test"}}'
```

### 3Ô∏è‚É£ OpenClaw Agent (Full AI Routing)

```bash
# Start OpenClaw in agent mode
cd openclaw
npm install  # First time only
node openclaw.mjs agent --mode rpc --json

# Then use OpenClaw's interface or voice to:
# - "Search YouTube for tutorials"
# - "What is machine learning?"
# - "Play music on YouTube"
# etc.
```

OpenClaw will automatically route requests to REM services.

### 4Ô∏è‚É£ Python API (for Custom Scripts)

```python
import asyncio
from server.rem_openclaw_adapter import get_adapter, RemService

async def main():
    adapter = get_adapter()
    
    # LLM
    result = await adapter.call_service(
        RemService.LLM_GENERATE,
        {"prompt": "What is Python?"},
        timeout=30
    )
    print(result["result"]["text"])
    
    # Web control
    result = await adapter.call_service(
        RemService.WEB_CONTROL,
        {"text": "search youtube for python"},
        timeout=10
    )
    print(result["result"]["detail"]["message"])

asyncio.run(main())
```

---

## üìÅ Project Structure

```
Rem_project/
‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îú‚îÄ‚îÄ rem_openclaw_adapter.py      ‚Üê Core OpenClaw bridge
‚îÇ   ‚îú‚îÄ‚îÄ openclaw_bridge.py           ‚Üê HTTP RPC server
‚îÇ   ‚îú‚îÄ‚îÄ main_chat.py                 ‚Üê Voice assistant loop
‚îÇ   ‚îî‚îÄ‚îÄ process/
‚îÇ       ‚îú‚îÄ‚îÄ web_control.py           ‚Üê YouTube/Google intent detection
‚îÇ       ‚îú‚îÄ‚îÄ llm_funcs/llm_scr.py    ‚Üê LLM wrapper
‚îÇ       ‚îú‚îÄ‚îÄ tts_func/sovits_ping.py ‚Üê TTS wrapper
‚îÇ       ‚îî‚îÄ‚îÄ asr_func/asr_push_to_talk.py ‚Üê ASR wrapper
‚îÇ
‚îú‚îÄ‚îÄ openclaw/                        ‚Üê Full OpenClaw repo
‚îÇ   ‚îî‚îÄ‚îÄ skills/
‚îÇ       ‚îî‚îÄ‚îÄ rem-local/
‚îÇ           ‚îî‚îÄ‚îÄ SKILL.md   ‚Üê REM skill definition for OpenClaw
‚îÇ
‚îú‚îÄ‚îÄ rem                              ‚Üê Python CLI entry point
‚îú‚îÄ‚îÄ rem.bat                          ‚Üê Windows batch wrapper
‚îú‚îÄ‚îÄ rem.ps1                          ‚Üê PowerShell wrapper
‚îÇ
‚îú‚îÄ‚îÄ character_config.yaml            ‚Üê REM configuration
‚îú‚îÄ‚îÄ chat_history.json                ‚Üê Conversation memory
‚îÇ
‚îî‚îÄ‚îÄ Documentation files...
```

---

## ‚öôÔ∏è Configuration

### REM Settings (`character_config.yaml`)

```yaml
# Language model
model:
  name: "llama3.2"
  ollama_url: "http://127.0.0.1:11434"
  temperature: 0.7

# Text-to-speech (optional)
tts:
  enabled: true
  sovits_url: "http://127.0.0.1:9880"

# Speech recognition
asr:
  model: "base.en"
  language: "en"

# Web browsing
web_control:
  enabled: true
  browser: "default"  # or "chrome", "firefox", etc.
```

### Bridge Settings

Edit `server/openclaw_bridge.py` or pass command-line args:

```bash
rem bridge --host 127.0.0.1 --port 8765
```

---

## üß™ Testing & Verification

### Test 1: Basic Health Check
```bash
rem health
# Expected: ‚úÖ OLLAMA: Running, ‚ùå SOVITS: Down (or ‚úÖ if available)
```

### Test 2: LLM Generation
```bash
rem llm "Hello, who are you?"
# Should respond with LLM-generated text
```

### Test 3: Web Control
```bash
rem web "search youtube for python programming"
# Browser should open with YouTube search results
```

### Test 4: HTTP API
```bash
# Start bridge in another terminal
rem bridge

# Test endpoint
curl http://127.0.0.1:8765/health | python -m json.tool
# Should return service status
```

### Test 5: Intent Detection
```bash
python -c "
from server.process.web_control import detect_intent
test_phrases = [
    'open youtube and search for python glasses',
    'google what is machine learning'
]
for phrase in test_phrases:
    intent, query = detect_intent(phrase)
    print(f'{phrase} -> {intent}: {query}')
"
```

---

## üîß Troubleshooting

### "rem command not found"
Use full path:
```bash
python rem llm "test"
```

### "Ollama not found"
Make sure Ollama is running:
```bash
ollama serve  # In another terminal
```

### "SoVITS not available"
TTS is optional. System works fine without it. To enable:
1. Start GPT-SoVITS service
2. Verify at http://127.0.0.1:9880
3. Update `character_config.yaml`

### "Bridge connection refused"
Make sure it's running:
```bash
rem bridge  # In another terminal
```

### "OpenClaw not found"
Already included in `./openclaw/` directory.

---

## üìä Service Status

| Feature | Status | When? |
|---------|--------|-------|
| **LLM Generation** | ‚úÖ Ready | Always |
| **Chat with History** | ‚úÖ Ready | Always |
| **Text-to-Speech** | ‚ö†Ô∏è Optional | If SoVITS running |
| **Speech Recognition** | ‚úÖ Ready | When recording |
| **Web Control** | ‚úÖ Ready | Always |
| **Voice Interaction** | ‚úÖ Ready | When mic available |
| **HTTP Bridge** | ‚úÖ Ready | When `rem bridge` running |
| **OpenClaw Agent** | ‚úÖ Ready | When `node openclaw.mjs` running |

---

## üéØ Common Use Cases

### Use Case 1: Ask a Question
```bash
rem llm "What is the capital of France?"
```

### Use Case 2: Get YouTube Results
```bash
rem web "search youtube for how to learn python"
# Opens browser with results
```

### Use Case 3: Have a Conversation
```bash
rem llm-chat "Tell me about AI"
rem llm-chat "Can you explain that differently?"  # Remembers context
```

### Use Case 4: Voice Input
```bash
rem voice listen
# Speak into microphone
# System transcribes and responds
```

### Use Case 5: Full OpenClaw Routing
```bash
cd openclaw
node openclaw.mjs agent --mode rpc

# Then tell it anything via OpenClaw's interface
# It automatically routes requests to REM skills
```

---

## üìà Performance & Limitations

### Performance
- **LLM Response:** 1-10 seconds (depends on system/model)
- **Web Control:** <1 second
- **Speech Recognition:** 2-5 seconds per sentence
- **Text-to-Speech:** 1-3 seconds per sentence (if available)

### Limitations
- Requires **Python 3.8+**
- Requires **Ollama** running locally (8GB+ RAM recommended)
- Optional **SoVITS** for TTS (GPU recommended)
- Windows/Mac/Linux compatible
- Best with >= 4 CPU cores

---

## üöÄ Advanced Topics

### Running as a Service
```bash
# Windows: Use Task Scheduler to run rem voice listen or rem bridge
# Linux/Mac: Use systemd or launchd
```

### Custom Skills
Add new services to `server/rem_openclaw_adapter.py`:

```python
class RemService(Enum):
    YOUR_CUSTOM_SKILL = "your_custom_skill"

# Then implement handler:
async def _your_custom_skill(self, params, timeout):
    # Your implementation
    pass
```

### Docker Deployment
Container ready (uses Python 3.8+ base image with dependencies).

### Environment Variables
```bash
export OLLAMA_URL=http://custom-host:11434
export SOVITS_URL=http://custom-host:9880
export CHAR_CONFIG=./custom_config.yaml
```

---

## üìû Support

### Check Health Status
```bash
rem health
```

### Enable Debug Logging
```bash
# Set environment variable
export LOG_LEVEL=DEBUG

# Then run command
rem llm "test"
```

### View Documentation
- **Quick Start:** [QUICKSTART_OPENCLAW_REM.md](./QUICKSTART_OPENCLAW_REM.md)
- **Full Guide:** [OPENCLAW_REM_INTEGRATION_COMPLETE.md](./OPENCLAW_REM_INTEGRATION_COMPLETE.md)
- **OpenClaw Skill:** [openclaw/skills/rem-local/SKILL.md](./openclaw/skills/rem-local/SKILL.md)

---

## üìú License & Attribution

- **REM:** This project
- **OpenClaw:** https://github.com/openclaw/openclaw (MIT License)
- **Ollama:** https://ollama.ai
- **Whisper:** OpenAI
- **GPT-SoVITS:** Open source
- **FastAPI:** https://fastapi.tiangolo.com

---

## ‚ú® Summary

You now have a **complete, production-ready AI assistant** that:

‚úÖ Runs entirely locally (no cloud dependencies)  
‚úÖ Understands voice and text commands  
‚úÖ Integrates with OpenClaw for advanced routing  
‚úÖ Provides an HTTP API for external tools  
‚úÖ Offers Python API for custom scripts  
‚úÖ Maintains conversation history  
‚úÖ Controls web browser (YouTube/Google)  

**Start:** `rem health` ‚Üí `rem voice listen` ‚Üí Talk to your computer!

---

**Questions or issues?** Check the documentation files or review the code in `server/rem_openclaw_adapter.py`.
